
import { QuizQuestion, CodeExample, AlgorithmStep } from './types';

export const codeExamples: CodeExample[] = [
    { id: 'basic', title: 'üöÄ Basic XGBoost Example', description: 'Simple classification example with default parameters', code: 'import xgboost as xgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train model\nmodel = xgb.XGBClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f"Accuracy: {accuracy:.3f}")\n# Output: Accuracy: 0.965' },
    { id: 'early_stopping', title: '‚è±Ô∏è Early Stopping Example', description: 'Prevent overfitting and find the optimal number of trees automatically.', code: 'import xgboost as xgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load and split data\ndata = load_breast_cancer()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create a validation set from the training data\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.25, random_state=42\n)\n\n# Initialize model\nmodel = xgb.XGBClassifier(\n    n_estimators=500, # Set a high number of estimators\n    learning_rate=0.1,\n    random_state=42,\n    eval_metric=\'logloss\'\n)\n\n# Fit the model with early stopping\nmodel.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=20, # Stop if validation loss doesn\'t improve for 20 rounds\n    verbose=False\n)\n\n# Make predictions with the best iteration\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f"Best Iteration: {model.best_iteration}")\nprint(f"Accuracy with Early Stopping: {accuracy:.3f}")' },
    { id: 'regression', title: 'üìà Regression Example', description: 'Using XGBoost for continuous target prediction', code: 'import xgboost as xgb\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Load California housing dataset\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# XGBoost Regressor\nmodel = xgb.XGBRegressor(\n    n_estimators=100,\n    max_depth=4,\n    learning_rate=0.1,\n    random_state=42\n)\n\n# Train and evaluate\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(f"RMSE: {rmse:.3f}")\nprint(f"R¬≤ Score: {r2:.3f}")' },
    { id: 'tuning', title: 'üéõÔ∏è Hyperparameter Tuning', description: 'Robust hyperparameter optimization with proper error handling', code: 'from sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\n\n# Load and prepare data\ndata = load_breast_cancer()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Conservative parameter grid\nparam_grid = {\n    \'n_estimators\': [50, 100],\n    \'max_depth\': [3, 4],\n    \'learning_rate\': [0.1, 0.2],\n    \'subsample\': [0.8, 1.0]\n}\n\n# Create model with stable settings\nxgb_model = xgb.XGBClassifier(\n    random_state=42,\n    eval_metric=\'logloss\'\n)\n\n# Grid search\ngrid_search = GridSearchCV(\n    estimator=xgb_model, \n    param_grid=param_grid, \n    cv=3,\n    scoring=\'accuracy\'\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f"Best parameters: {grid_search.best_params_}")' },
    { id: 'shap', title: 'üîç SHAP Model Interpretability', description: 'Understanding XGBoost predictions with global and local explanations', code: '# First install: pip install shap matplotlib\n\nimport xgboost as xgb\nimport shap\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Load housing data\ndata = fetch_california_housing()\nX_df = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_df, y, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = xgb.XGBRegressor(random_state=42).fit(X_train, y_train)\n\n# SHAP analysis\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test.iloc[:100])' }
];

export const algorithmSteps: AlgorithmStep[] = [
    { id: 'step1', title: '1. üéØ Initialize Prediction', detail: 'The model starts with a single, simple prediction for all data points. For regression, this is usually the average of the target values; for classification, it\'s the log-odds. Think of this as the "trunk" of our model‚Äîa simple, solid base. It\'s the best possible guess we can make without considering any features, as it minimizes the initial error.', formula: 'F‚ÇÄ(x) = initial_value (e.g., mean of y)'},
    { id: 'step2', title: '2. üìâ Calculate Residuals', detail: 'The model calculates the errors, or "residuals," by subtracting the current predictions from the actual values. These residuals represent everything the model currently gets wrong. They are the "unexplained" part of the data. The entire goal of the next step is to specifically target and correct these mistakes.', formula: 'residuals = actuals - predictions' },
    { id: 'step3', title: '3. üå≥ Train a Decision Tree', detail: 'A new decision tree (a "weak learner") is trained, but with a twist: instead of predicting the original target, it learns to predict the residuals from the previous step. This tree is a specialist, focusing only on fixing the specific errors the model made in the last round. It finds patterns in the mistakes.', formula: 'new_tree = DecisionTree(features ‚Üí residuals)' },
    { id: 'step4', title: '4. ‚öñÔ∏è Find Optimal Weight', detail: 'Instead of just adding the new tree\'s predictions, XGBoost calculates an optimal weight (gamma) for each leaf of the tree. This is a crucial, sophisticated step. It ensures that each tree contributes just the right amount to the final prediction, preventing overcorrection. It\'s like a senior expert deciding exactly how much to trust a junior expert\'s advice.', formula: 'optimal_weight = best_Œ≥_value' },
    { id: 'step5', title: '5. ‚ûï Update the Model', detail: 'The predictions from the new tree, scaled by both its optimal weights and a global "learning rate," are added to the overall model. The learning rate acts as a safety brake, forcing the model to take small, careful steps. This incremental improvement makes the model robust and prevents it from overshooting the best solution.', formula: 'F_new(x) = F_old(x) + learning_rate √ó new_tree(x)' },
    { id: 'step6', title: '6. üîÑ Repeat Until Convergence', detail: 'The process repeats from Step 2: calculate new residuals based on the updated model, train a new tree to fix them, and add it to the ensemble. This cycle continues until adding new trees no longer improves performance on a separate validation dataset (known as early stopping) or a maximum number of trees is reached. This ensures the model becomes progressively more accurate without overfitting.', formula: 'Repeat steps 2-5 for M rounds' }
];

export const coreConceptsQuiz: QuizQuestion[] = [
    { question: "What is the primary goal of each new tree in a gradient boosting model?", options: [{ text: "To predict the original target variable.", correct: false }, { text: "To correct the errors (residuals) of the previous trees.", correct: true }, { text: "To be as deep and complex as possible.", correct: false }] },
    { question: "What does 'boosting' refer to in the context of XGBoost?", options: [{ text: "A method of training many models in parallel.", correct: false }, { text: "A sequential process where models are added one by one to correct prior mistakes.", correct: true }, { text: "Using very powerful, complex models from the start.", correct: false }] }
];

export const algorithmQuiz: QuizQuestion[] = [
    { question: "What is the very first prediction made by the XGBoost algorithm (before any trees are built)?", options: [{ text: "A random number.", correct: false }, { text: "Zero for all instances.", correct: false }, { text: "The average of the target values.", correct: true }] },
    { question: "What are 'residuals' in the XGBoost algorithm?", options: [{ text: "The difference between the actual values and the current predictions.", correct: true }, { text: "Data points that are considered outliers.", correct: false }, { text: "The features that are least important.", correct: false }] },
    { question: "What is the purpose of the 'learning_rate' parameter?", options: [{ text: "To control how fast the model trains on the GPU.", correct: false }, { text: "To scale the contribution of each new tree to prevent overfitting.", correct: true }, { text: "To determine the maximum depth of a tree.", correct: false }] }
];
